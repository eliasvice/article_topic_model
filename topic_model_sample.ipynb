{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vice Articles Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras, numpy as np\n",
    "from keras.layers import Embedding, Dense, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_A small sample dataset to train and test the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0002_part_00 to data/article_data_0002_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0003_part_00 to data/article_data_0003_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0004_part_00 to data/article_data_0004_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0001_part_00 to data/article_data_0001_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0000_part_00 to data/article_data_0000_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0005_part_00 to data/article_data_0005_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0008_part_00 to data/article_data_0008_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0006_part_00 to data/article_data_0006_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0010_part_00 to data/article_data_0010_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0007_part_00 to data/article_data_0007_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0009_part_00 to data/article_data_0009_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0011_part_00 to data/article_data_0011_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0012_part_00 to data/article_data_0012_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0013_part_00 to data/article_data_0013_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0014_part_00 to data/article_data_0014_part_00\n",
      "download: s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/article_data_0015_part_00 to data/article_data_0015_part_00\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://topic-model-hack/hackweek/articles_with_1000_plus_topics/ ./data/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.read_csv(f, sep='|', engine='python', header=None) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189295, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(\"./data/article_data*\")\n",
    "data = pd.concat([pd.read_csv(f, sep='|', engine='python', header=None) for f in files])\n",
    "data.columns = ['article_id', 'body', 'topic']\n",
    "data = data[~data.body.isnull()]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/article_data_*'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0e817ff9c6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/article_data_*\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# data.columns = ['article_id', 'url_fragment', 'first_published', 'body', 'topic']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[1;32m   1058\u001b[0m                                      engine=engine))\n\u001b[0;32m-> 1059\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2072\u001b[0m         f, handles = _get_handle(f, mode, encoding=self.encoding,\n\u001b[1;32m   2073\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2074\u001b[0;31m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/article_data_*'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_loc = \"./data/article_data_*\"\n",
    "data = pd.read_csv(data_loc, sep='|', engine='python', names=['article_id','body', 'topic'], header=None)\n",
    "# data.columns = ['article_id', 'url_fragment', 'first_published', 'body', 'topic']\n",
    "data = data[~data.body.isnull()]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'body', 'topic'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = 200000\n",
    "max_seq_len = 2000\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(np.floor(data.shape[0] * .8))\n",
    "\n",
    "\n",
    "# train_x = data[\"body\"][0:train_size]\n",
    "# train_y = data[\"topic\"][0:train_size]\n",
    "\n",
    "# test_x = data[\"body\"][train_size:]\n",
    "# test_y = data[\"topic\"][train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"body\"]\n",
    "y = data[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 0.00011093795398716289)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = list(y.unique())\n",
    "y_encoded = [topics.index(topic) for topic in y] \n",
    "\n",
    "n_classes = len(topics)\n",
    "n_classes, sum(y.value_counts() <= 10)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for the model\n",
    "* Tokenizing the text - Identifying unique words, creating a dictionary and counting their frequency in the list of documents (texts) in the training data.\n",
    "* One-hot encoding the labels (topics)\n",
    "* Splitting the data into train and test(validation) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=nb_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = Tokenizer.texts_to_sequences(tokenizer, X)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "ydata = keras.utils.to_categorical(y_encoded)\n",
    "input_data = pad_sequences(sequences, maxlen=max_seq_len)\n",
    "\n",
    "Xtrain, Xvalid, ytrain, yvalid = train_test_split(input_data, ydata, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('model/tokenizer.pickle', 'rb') as handle:\n",
    "    tok = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Model definition and training_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 2000, 128)         120826368 \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 80)                66880     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 282)               22842     \n",
      "=================================================================\n",
      "Total params: 120,916,090\n",
      "Trainable params: 120,916,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embedding_vector_length, input_length=max_seq_len, embeddings_initializer='glorot_normal', \n",
    "                    embeddings_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(LSTM(80))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120826368 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151436 samples, validate on 37859 samples\n",
      "Epoch 1/5\n",
      " 41984/151436 [=======>......................] - ETA: 32:06 - loss: 211.9265 - categorical_accuracy: 0.0691"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, validation_data=(Xvalid, yvalid), nb_epoch=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = (\"\"\"Why Some Rappers Just Aren't Good Live True fact: lots of successful rappers don't need a great live show to get where they are. Let them live! Danny Brown, who is a good live rapper, performs live.It doesn't take long for the average person to develop a healthy skepticism of rap shows. Aside from the usual live venue problems (guest list confusion, aggro bouncers, expensive drinks), rap has its own special set of bullshit. There are too many opening acts, a rotating list of five local struggle rappers who never graduate to headliner status. There are too many people and it's always mostly dudes who often feel a way about you breathing their air. The shows run hours behind schedule. If you make it that far, the headliner is often not very good. If they even know all their lyrics, they're spitting them on top of their own recorded vocals.It's that last sin, rappers not really even rapping, that seems to offend the most. And it's not just dillettante hip-hop fans who complain; it's something I hear all the time as a reason why my friends don't want to go see a rapper whose music they love perform their songs in real life. It's less of a concert and more of a spectacle.So why does this happen? Why do some rappers seem to phone in their live shows? It's not like live hip-hop is dead. After all, active showmen like Action Bronson and Danny Brown have clearly benefitted from the work they put into their stage presence. Why don't all rappers do the same thing? The short answer is you can become a famous rapper without doing very many shows, per se.The assumed path to success for musicians today involves recording music, booking to play your music and building a fan base that will buy your music and see your shows. Increasingly, convincing brand managers that your music will sell products, but that's part of the same performance-heavy equation. And this is proven method across genres: rock, country, disco and, yeah, even rap.But rappers often take an alternate path, one that relies way less on stagework. Instead of recording an album and hustling a live show, they pick one song and push it to as many DJ's as they can. If the record is good, it gets added to the rotation on the local radio station and in the clubs around the way. And if the record is really good (and if they have good people behind you), it will get added to more stations and played at more clubs, further and further from the local market.As the single gets momentum and their name gets out there, promoters far and wide will book them to perform at clubs. But this isn't a full show, it's a quick set at a club based around a hit record. If the song is really popular, the artist's presence is just gravy. Everybody will know all the words and the club would have gone off whether or not they were there. Nobody's worried about the rapper nailing the hook.A rapper with a big single can make an insane amount of money doing club sets. Rappers are fitting into a nightlife culture that relies on paying a diverse range of celebrities to show up as a way to entice people to come out; they can earn their money just by being in the building. And everything resembling a metropolitan area has a nightlife destination where a rapper might get booked if their record is getting spins. There are way fewer ideal places for full-fledged rap shows, and that number gets smaller after factoring in many venues' inherent biases against hip-hop events. An artist that can get a stack for a club appearance in, say, Albany, GA is not stressed about their stage presence and breath control.All this said, should we even expect old school showmanship and rappin-ass rappin from these dudes? If the music makes a mob of excited people jump up and down and yell lyrics at each other, that's not a bad thing (nor is it a new concept).But that's the reason why some successful emcees don't have great live shows: they never needed one to get where they are.Skinny Friedman has unmatchable stage presence when he blogs. He's on Twitter - @skinny412\"\"\", 'rap')\n",
    "\n",
    "ex2 = (\"\"\"‘Black-ish’ Addressed Postpartum Depression Better Than My Doctor It’s the PSA of my goddamn dreams. At the height of my postpartum depression and anxiety, I had been to the emergency room twice for palpitations I perceived to be a heart attack. I also had an MRI for migraines I was convinced were the result of a tumor. I often lay awake watching my daughter's monitor for hands that would snatch her from her crib. I called my dentist after hours and pushed for her to assure me my tooth pain was not an infection that would spread to my brain. Most nights before turning out the lights I asked my husband Dan to assure me I wouldn't die in my sleep. On one particular evening, I was lying in bed with my legs against our headboard, trying to take the pressure off of an ache in my calf that I was pretty sure was a blood clot slowly making its way to my lungs. Dan slid into bed and I turned my head to him, panicked tears already rolling down my face. 'Do you think…?' Before I finished he said, 'Yes, I will see you in the morning. I promise.' He caught my eye and we both started laughing. It wasn't exactly funny, but even in the midst of my very real distress, I knew my thought patterns weren't typical. Sometimes finding humor in it made it feel less overwhelming. Tracee Ellis Ross reinforced that in last night's episode of  Black-ish when her character Rainbow ('Bow') and her husband, Dre (played by Anthony Johnson), take a quiz about postpartum depression. Dre reads the first question: 'Do you feel sad, hopeless, overwhelmed, empty?' Through tears, Bow responds: 'Ah…well, I feel sad and I feel hopeless and I feel overwhelmed, but I don't feel empty, so I guess it's a no for me.' It's a moment of comic relief in the scene—while Rainbow is clearly depressed, this moment pokes fun at her just falling short of being the total package of PPD misery. 'I think that comedy can help us shine a light on important mental health issues, when it is done responsibly,' says Mike Fraser, a psychologist chief of staff at Behavioral Associates in New York. 'Comedy that aims for the easy laugh by poking fun at people struggling with real mental health issues obviously doesn't help. But if it can bring exposure to issues that millions of people battle—often in isolation—comedy can open the door for people to get an important conversation going and possibly even reach out for help.'  Watch this from Tonic:As Dan and I watched the episode, we were both especially struck by a scene in which Dre and Bow visit a doctor about her perceived condition. They discuss Bow's symptoms—anxiety, insomnia, crying and constant insecurity—and the doctor assures her they are not only normal, but treatable. Bow represents a huge number of women when she tells the doctor, 'I can get through this, I don't want medication.' Her doctor replies, 'postpartum depression is a mood disorder—it's not just something you can power through—and it's not something you should be ashamed of.'  It's the postpartum PSA of my goddamn dreams—one I wish they would play in place of the over simplified, condescending crap most women hear before exiting the hospital with a new baby. Dan points out that the scene represents a support system some women will never see in real life. And I knew he was right: My own experience with a doctor who listened to my symptoms and then put her hands on my knees and told me to go for a brisk walk was proof enough. Even though I had dealt with depression and anxiety for most of my adult life, her patronization made me second guess what I knew to be facts. Lucky for me, by the time I made it to the car my support system (hi mom) was on other end of the phone while I screamed, 'Fuck that lady!' I knew that exercise couldn't 'cure' what science has shown to be chemical. But I was pissed for the people who would accept her words as truth. Hours of therapy and one daily dose of Lexapro later, I still struggle with bouts of depression and many of my impulses from those early days of motherhood have hung around. But each night when Dan locks the front door and I walk over to touch it—a habit rooted in obsessive compulsive tendencies—we both laugh. 'I know, I know,' I say, smiling as I reach out my hand. It's a necessary ritual for me to feel at ease, but the lightness we sometimes inject into it makes it feel less clinical. And that's what  Black-ish offers its viewers when Dre cozies up next to Bow on the couch and rattles off questions from a women's magazine quiz, giving her props on her near perfect score (and while that equates to depressed as hell, Bow is never one to bomb a quiz of any type). There were some scenes that were a little too neatly tied up, particularly one where Bow's mother-in-law Ruby apologizes for her vast misconceptions about PPD (which included comments like, 'I didn't go to some quack doctor because I was mentally ill with some made up disease'). The turnaround between her blatant ignorance and acceptance is a bit quick and, sadly, unrealistic, but it sticks with the show's intent of bringing a serious issue to an accessible 30-minute comedic platform. After all, I realized, most people don't discuss mental health on the regular as we do in our household. Shows like  Black-ish might be the open door they've been needing to walk through.Read This Next: Chrissy Teigen, Postpartum Depression, and Trump\"\"\", 'depression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts([example[0]])\n",
    "sequences = Tokenizer.texts_to_sequences(tok, [ex2[0]])\n",
    "inp = pad_sequences(sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/topic_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'depression' in topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
