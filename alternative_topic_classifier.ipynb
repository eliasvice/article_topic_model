{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have TensorFlow version 1.7.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras, numpy as np\n",
    "from keras.layers import Embedding, Dense, LSTM, GRU, Conv1D, Reshape\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "import pickle\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189295, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(\"./data/article_data*\")\n",
    "data = pd.concat([pd.read_csv(f, sep='|', engine='python', header=None) for f in files])\n",
    "data.columns = ['article_id', 'body', 'topic']\n",
    "data = data[~data.body.isnull()]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 160900\n",
      "Test size: 28395\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "train_size = int(len(data) * .85)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body  = data['body'][:train_size]\n",
    "train_topic = data['topic'][:train_size]\n",
    "\n",
    "test_body = data['body'][train_size:]\n",
    "test_topic = data['topic'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words  = 10000\n",
    "try:\n",
    "    with open('model/tokenizer.pickle', 'rb') as handle:\n",
    "        tokenize = pickle.load(handle)\n",
    "except Exception:\n",
    "    tokenize = text.Tokenizer(num_words=nb_words)\n",
    "    tokenize.fit_on_texts(train_body) # only fit on train\n",
    "    with open('model/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "x_train = tokenize.texts_to_matrix(train_body)\n",
    "x_test = tokenize.texts_to_matrix(test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  282 less than 10 % 0.0001305158483530143\n",
      "x_train shape: (160900, 20000)\n",
      "x_test shape: (28395, 20000)\n",
      "y_train shape: (160900, 282)\n",
      "y_test shape: (28395, 282)\n"
     ]
    }
   ],
   "source": [
    "topics = list(train_topic.unique())\n",
    "ytrain_encoded = [topics.index(topic) for topic in train_topic] \n",
    "ytest_encoded = [topics.index(topic) for topic in test_topic]\n",
    "\n",
    "y_train = keras.utils.to_categorical(ytrain_encoded)\n",
    "y_test = keras.utils.to_categorical(ytest_encoded)\n",
    "n_classes = len(topics)\n",
    "print('classes: ', n_classes, 'less than 10 %', str(sum(train_topic.value_counts() <= 10)/train_topic.shape[0]))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 15\n",
    "\n",
    "word_index = tokenize.word_index\n",
    "\n",
    "\n",
    "embedding_dim = embedding_vector_length = 64\n",
    "sequence_length = max_seq_len = 1000\n",
    "\n",
    "vocabulary_size = nb_words\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model():\n",
    "    \n",
    "    global graph\n",
    "    graph = tf.get_default_graph()\n",
    "    print('building DNN model: ')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(20000,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(\"model built: \", model.summary())\n",
    "    return model\n",
    "\n",
    "def cnn_model():\n",
    "    print('building CNN model: ')\n",
    "    from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "    from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.models import Model\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from data_helpers import load_data\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(len(word_index)+1,\n",
    "#                     embedding_vector_length, input_length=max_seq_len, embeddings_initializer='glorot_normal', \n",
    "#                         embeddings_regularizer=keras.regularizers.l1(0.01)))\n",
    "    \n",
    "#     model.add(Reshape((max_seq_len, embedding_vector_length,1)))\n",
    "\n",
    "    inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "    reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='glorot_normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='glorot_normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='glorot_normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(n_classes, activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print(\"model built: \", model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building DNN model: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               10240512  \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 282)               144666    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 282)               0         \n",
      "=================================================================\n",
      "Total params: 10,385,178\n",
      "Trainable params: 10,385,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model built:  None\n",
      "Train on 144810 samples, validate on 16090 samples\n",
      "Epoch 1/15\n",
      "144810/144810 [==============================] - 43s 297us/step - loss: 3.2072 - acc: 0.2705 - val_loss: 2.4879 - val_acc: 0.3562\n",
      "Epoch 2/15\n",
      "144810/144810 [==============================] - 42s 291us/step - loss: 2.0667 - acc: 0.3811 - val_loss: 2.3788 - val_acc: 0.3542\n",
      "Epoch 3/15\n",
      "144810/144810 [==============================] - 43s 294us/step - loss: 1.7152 - acc: 0.4216 - val_loss: 2.4042 - val_acc: 0.3465\n",
      "Epoch 4/15\n",
      "144810/144810 [==============================] - 42s 291us/step - loss: 1.5354 - acc: 0.4392 - val_loss: 2.4421 - val_acc: 0.3421\n",
      "Epoch 5/15\n",
      "144810/144810 [==============================] - 42s 293us/step - loss: 1.4304 - acc: 0.4498 - val_loss: 2.4734 - val_acc: 0.3375\n",
      "Epoch 6/15\n",
      "144810/144810 [==============================] - 42s 291us/step - loss: 1.3698 - acc: 0.4554 - val_loss: 2.5032 - val_acc: 0.3372\n",
      "Epoch 7/15\n",
      "144810/144810 [==============================] - 42s 293us/step - loss: 1.3230 - acc: 0.4599 - val_loss: 2.5306 - val_acc: 0.3370\n",
      "Epoch 8/15\n",
      "144810/144810 [==============================] - 42s 291us/step - loss: 1.2902 - acc: 0.4631 - val_loss: 2.5513 - val_acc: 0.3347\n",
      "Epoch 9/15\n",
      "144810/144810 [==============================] - 42s 293us/step - loss: 1.2671 - acc: 0.4652 - val_loss: 2.5748 - val_acc: 0.3359\n",
      "Epoch 10/15\n",
      "144810/144810 [==============================] - 42s 291us/step - loss: 1.2450 - acc: 0.4648 - val_loss: 2.5898 - val_acc: 0.3357\n",
      "Epoch 11/15\n",
      "144810/144810 [==============================] - 42s 293us/step - loss: 1.2282 - acc: 0.4661 - val_loss: 2.6201 - val_acc: 0.3346\n",
      "Epoch 12/15\n",
      "144810/144810 [==============================] - 42s 291us/step - loss: 1.2145 - acc: 0.4677 - val_loss: 2.6208 - val_acc: 0.3370\n",
      "Epoch 13/15\n",
      "144810/144810 [==============================] - 42s 293us/step - loss: 1.1991 - acc: 0.4697 - val_loss: 2.6282 - val_acc: 0.3354\n",
      "Epoch 14/15\n",
      "144810/144810 [==============================] - 42s 290us/step - loss: 1.1883 - acc: 0.4682 - val_loss: 2.6276 - val_acc: 0.3334\n",
      "Epoch 15/15\n",
      "144810/144810 [==============================] - 43s 299us/step - loss: 1.1752 - acc: 0.4706 - val_loss: 2.6278 - val_acc: 0.3356\n"
     ]
    }
   ],
   "source": [
    "model = dnn_model()\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28395/28395 [==============================] - 4s 131us/step\n",
      "Test score: 2.3592046725328046\n",
      "Test accuracy: 0.38112343731932413\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/dnn/topic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/topics.txt', 'a') as f:\n",
    "    f.write(json.dumps(list(train_topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throwback Thursday: Earl Weaver Arrives in October, Is Promptly Ejected In  ... \n",
      "Actual label:baseball\n",
      "Predicted label: baseball\n",
      "Confidence: 59.11%\n",
      "\n",
      "'The Littlest Memory,' Today's Comic by Leslie Stein Award winning cartooni ... \n",
      "Actual label:post-regular\n",
      "Predicted label: post-regular\n",
      "Confidence: 86.81%\n",
      "\n",
      "An Ad Company Is Flying Surveillance Drones Over Los Angeles And there's no ... \n",
      "Actual label:privacy\n",
      "Predicted label: motherboard show\n",
      "Confidence: 49.33%\n",
      "\n",
      "Animated Music Video Turns a Phosphorescent Garden into Rave Central Animat ... \n",
      "Actual label:dance\n",
      "Predicted label: music videos\n",
      "Confidence: 30.76%\n",
      "\n",
      "Trevor Booker Beats Shot Clock With Ridiculous Backwards Shot Trevor Booker ... \n",
      "Actual label:basketball\n",
      "Predicted label: basketball\n",
      "Confidence: 61.48%\n",
      "\n",
      "Dead Calm This is chapter 21 of Robert Young Pelton and Tim Freccia's spraw ... \n",
      "Actual label:violence\n",
      "Predicted label: vice magazine\n",
      "Confidence: 32.26%\n",
      "\n",
      "Daily Horoscope: November 03, 2016 Daily horoscopes and astrology for Aquar ... \n",
      "Actual label:astro guide\n",
      "Predicted label: astro guide\n",
      "Confidence: 100.0%\n",
      "\n",
      "A Prized Pokémon Card from Transgender Teen Hero Gavin Grimm There's someth ... \n",
      "Actual label:vice magazine\n",
      "Predicted label: vice magazine\n",
      "Confidence: 68.56%\n",
      "\n",
      "Robert Downey Jr. Gave a Cute 7-Year-Old Boy an Iron Man-Like Bionic Arm Yo ... \n",
      "Actual label:machines\n",
      "Predicted label: motherboard show\n",
      "Confidence: 72.74%\n",
      "\n",
      "Somalia: The Fight Against al Shabaab Simon Ostrovsky traveled to Somalia t ... \n",
      "Actual label:terrorism\n",
      "Predicted label: africa\n",
      "Confidence: 20.06%\n",
      "\n",
      "years & years' olly alexander tells glastonbury to “shove a rainbow in fear ... \n",
      "Actual label:music news\n",
      "Predicted label: thump news\n",
      "Confidence: 46.51%\n",
      "\n",
      "Trump’s Massive Expansion of the Global Gag Rule Will Kill Women, Advocates ... \n",
      "Actual label:politics\n",
      "Predicted label: politics\n",
      "Confidence: 38.4%\n",
      "\n",
      "How I Earned $200K Making Spaghetti Sauce in the Amazon Jungle I don’t own  ... \n",
      "Actual label:amazon\n",
      "Predicted label: travel\n",
      "Confidence: 24.9%\n",
      "\n",
      "Thousands March for Abortion Rights in Ireland Just days after the Irish go ... \n",
      "Actual label:uk\n",
      "Predicted label: abortion\n",
      "Confidence: 69.68%\n",
      "\n",
      "Teams To Watch At Euro 2016: England May Surprise Instead of Disappoint Eng ... \n",
      "Actual label:england\n",
      "Predicted label: soccer\n",
      "Confidence: 52.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    max_prob = np.argmax(prediction)\n",
    "    predicted_label = topics[max_prob]\n",
    "    print(test_body.iloc[i][:75], \"... \")\n",
    "    print('Actual label:' + test_topic.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label )\n",
    "    print(\"Confidence: \" + str(np.round(prediction[:, max_prob][0] * 100, 2)) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 282)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
